[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AAGI-Resource-Library",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "pages/count_data.html",
    "href": "pages/count_data.html",
    "title": "Count Data!",
    "section": "",
    "text": "Hello\n\nprint(\"ligma\")\n\n[1] \"ligma\""
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html",
    "href": "pages/interpreting_outputs/summary_statistics.html",
    "title": "Summary Statistics",
    "section": "",
    "text": "Summary statistics are a key component of descriptive statistics, providing numerical measures and language to help summarise and describe the main features of a dataset. These include:\nTogether, these statistics help to simplify and summarise raw data, allowing for the identification of trends, patterns, and anomalies before moving on to more complex statistical modeling or hypothesis testing. By distilling data into key figures, summary statistics make it easier to compare variables, detect outliers, and understand potential relationships or dependencies within the data."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#mean",
    "href": "pages/interpreting_outputs/summary_statistics.html#mean",
    "title": "Summary Statistics",
    "section": "Mean",
    "text": "Mean\nThe mean is the average value, calculated by summing all data points and dividing by the total number of observations.\nThe mean provides a general idea of the typical value in a dataset. However, it can be influenced by extreme values (outliers), which can make it less reliable for datasets with skewed distributions."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#median",
    "href": "pages/interpreting_outputs/summary_statistics.html#median",
    "title": "Summary Statistics",
    "section": "Median",
    "text": "Median\nThe median is the middle value when the data is arranged in order. If there is an even number of values, the median is the average of the two middle numbers.\nThe median is less affected by outliers, making it a more reliable measure of central tendency for skewed datasets."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#mode",
    "href": "pages/interpreting_outputs/summary_statistics.html#mode",
    "title": "Summary Statistics",
    "section": "Mode",
    "text": "Mode\nThe mode is the most frequently occurring value in a dataset. A dataset can be:\n\nUnimodal (one mode)\nBimodal (two modes)\nMultimodal (multiple modes).\n\nIf no value repeats, there is no mode.\nThe mode identifies the most common value in the dataset, making it useful for analysing categorical or discrete numerical data."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#variance",
    "href": "pages/interpreting_outputs/summary_statistics.html#variance",
    "title": "Summary Statistics",
    "section": "Variance",
    "text": "Variance\nVariance measures how far the data points deviate from the mean.\nA higher variance indicates that the data points are more spread out, while a lower variance suggests that the data points are more tightly clustered around the mean."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#standard-deviation-sd",
    "href": "pages/interpreting_outputs/summary_statistics.html#standard-deviation-sd",
    "title": "Summary Statistics",
    "section": "Standard Deviation (SD)",
    "text": "Standard Deviation (SD)\nStandard deviation is the square root of the variance, representing the average distance of data points from the mean.\nA small standard deviation indicates that the values are consistently close to the mean, while a large standard deviation suggests greater variability or spread in the data."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#coefficient-of-variation-cv",
    "href": "pages/interpreting_outputs/summary_statistics.html#coefficient-of-variation-cv",
    "title": "Summary Statistics",
    "section": "Coefficient of Variation (CV)",
    "text": "Coefficient of Variation (CV)\nThe coefficient of variation is the standard deviation divided by the mean, expressed as a percentage. It is useful for comparing variability across datasets with different units or scales.\nA higher CV indicates greater relative variability, while a lower CV suggests more stable, consistent data."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#correlation-coefficient-pearsons-r",
    "href": "pages/interpreting_outputs/summary_statistics.html#correlation-coefficient-pearsons-r",
    "title": "Summary Statistics",
    "section": "Correlation Coefficient (Pearson’s r)",
    "text": "Correlation Coefficient (Pearson’s r)\nThe correlation coefficient measures the strength and direction of the linear relationship between two numerical variables. The value of r ranges from -1 to +1:\n\nr = +1: Perfect positive correlation (as one variable increases, the other increases).\nr = -1: Perfect negative correlation (as one variable increases, the other decreases).\nr = 0: No correlation (variables are unrelated).\n\nThis statistic helps assess whether and how strongly two variables are related, providing insight into whether one variable can predict changes in another."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#skewness",
    "href": "pages/interpreting_outputs/summary_statistics.html#skewness",
    "title": "Summary Statistics",
    "section": "Skewness",
    "text": "Skewness\nSkewness describes the symmetry of the data distribution:\n\nSymmetrical: A bell-shaped curve, typical of a normal distribution, with values clustered around the mean.\nRight-Skewed (Positive Skew): A distribution with a longer tail on the right, indicating more low values.\nLeft-Skewed (Negative Skew): A distribution with a longer tail on the left, indicating more high values.\n\nSkewness helps identify the direction of the data’s asymmetry and can give insights into the nature of the distribution."
  },
  {
    "objectID": "pages/interpreting_outputs/summary_statistics.html#additional-notes-on-interpreting-summary-statistics",
    "href": "pages/interpreting_outputs/summary_statistics.html#additional-notes-on-interpreting-summary-statistics",
    "title": "Summary Statistics",
    "section": "Additional Notes on Interpreting Summary Statistics",
    "text": "Additional Notes on Interpreting Summary Statistics\nWhen interpreting summary statistics, it’s important to consider multiple measures together rather than in isolation. This approach provides a fuller understanding of the data and helps prevent misleading conclusions. Here are a few examples:\n\nA larger range and higher standard deviation indicate greater variability in the data.\nA wide range with a small standard deviation may suggest the presence of outliers.\nIf the mean is greater than the median, the distribution is likely right-skewed.\nIf the mean is smaller than the median, the distribution is likely left-skewed.\nIf the mean and median are similar, the data is likely normally distributed.\n\nBy examining these statistics together, you can gain a more comprehensive understanding of your data’s structure and avoid drawing conclusions based on a single statistic."
  },
  {
    "objectID": "pages/interpreting_outputs/plots_and_figures.html",
    "href": "pages/interpreting_outputs/plots_and_figures.html",
    "title": "Plots and Figures",
    "section": "",
    "text": "In data analysis, visualisations are essential for providing a clear and intuitive understanding of complex data sets. Tools like histograms, box plots, and scatter plots help to uncover patterns, relationships, and trends that may not be immediately apparent from raw data alone. By presenting data graphically, visualisations allow analysts to spot outliers, assess distribution shapes, and interpret the strength and direction of correlations. This section will explore different types of visualisations used in descriptive statistics, explaining their purpose and how they help in drawing meaningful insights from data."
  },
  {
    "objectID": "pages/interpreting_outputs/plots_and_figures.html#histograms",
    "href": "pages/interpreting_outputs/plots_and_figures.html#histograms",
    "title": "Plots and Figures",
    "section": "Histograms",
    "text": "Histograms\nOverview: A histogram is a graphical representation of numerical data that organises values into intervals, known as bins, and displays how frequently each value occurs using bars. It provides a straightforward and powerful way to visualise the distribution of data, making it easier to identify patterns, trends, and anomalies.\nHistograms are particularly useful for understanding the shape and spread of a dataset. They help assess whether the data follows a normal distribution, with most values clustering around the centre, or if the data is skewed, indicating that values tend to concentrate at one extreme. Additionally, histograms are useful for spotting outliers (values that fall far outside the majority of the data points) which can have a significant impact on statistical analysis.\nInterpretation: When interpreting a histogram, focus on the height and position of the bars. Taller bars indicate that many data points fall within a particular range, while shorter bars suggest fewer data points in those intervals.\nIf the bars are evenly distributed around the centre, the data is likely normally distributed, resembling a bell curve. If the histogram has a rightward tail (longer bars on the right side), it suggests a right-skewed (positively skewed) distribution, where lower values are more frequent. Conversely, a leftward tail indicates a left-skewed (negatively skewed) distribution, where higher values dominate.\nHistograms can also reveal distinct subgroups or bimodal distributions if multiple peaks are visible, and they can highlight outliers that fall outside the main concentration of data."
  },
  {
    "objectID": "pages/interpreting_outputs/plots_and_figures.html#box-plots",
    "href": "pages/interpreting_outputs/plots_and_figures.html#box-plots",
    "title": "Plots and Figures",
    "section": "Box Plots",
    "text": "Box Plots\nOverview: A box plot, or box-and-whisker plot, is a graphical tool used to summarise the distribution of a dataset by highlighting key statistics, such as the median, quartiles, and potential outliers. It provides a compact yet effective way to visualise data spread and is particularly useful for comparing multiple datasets side by side.\nBox plots are especially valuable in exploratory data analysis. They make it easy to identify patterns, detect outliers, and assess the overall distribution of data without requiring extensive numerical summaries. Additionally, they are ideal for evaluating consistency in experimental data, comparing categories, and detecting anomalies, offering a straightforward approach to understanding the structure of a dataset.\nInterpretation: A box plot displays the interquartile range (IQR) and provides a visual summary of the dataset’s distribution. The box represents the IQR, containing the middle 50% of the data, with the line inside the box marking the median, or central value. The whiskers extend outward to show the range of most data points, typically bounded by the lower limit (Q1 − 1.5 × IQR) and the upper limit (Q3 + 1.5 × IQR). Any data points that fall outside this range are considered outliers and are displayed as individual dots.\nWhen interpreting a box plot, the position of the median within the box is important. If the median is closer to one end, it suggests a skewed distribution. A longer whisker on one side indicates that the data is more spread out in that direction. Outliers, visible as dots outside the whiskers, highlight unusual data points that may require further examination.\nComparing multiple box plots allows for an assessment of differences in IQR, median placement, and outliers, which can provide insights into the variation between datasets."
  },
  {
    "objectID": "pages/interpreting_outputs/plots_and_figures.html#scatter-plots",
    "href": "pages/interpreting_outputs/plots_and_figures.html#scatter-plots",
    "title": "Plots and Figures",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nOverview: A scatter plot is a graphical tool used to visualise the relationship between two numerical variables by displaying individual data points on a Cartesian plane. Each point represents a single observation, with its position determined by the values of both variables—typically plotted along the x-axis (independent variable) and y-axis (dependent variable). This makes scatter plots particularly useful for identifying patterns, correlations, and trends within a dataset.\nThe primary purpose of a scatter plot is to reveal the relationship between two numerical variables. It helps to identify potential correlations (positive, negative, or none), assess the strength of the relationship, and detect outliers or anomalies. Scatter plots are essential in statistical analysis for exploring relationships, making predictions, and determining whether a linear or non-linear trend exists between variables.\nInterpretation: Scatter plots provide key insights into the direction and strength of a relationship between the variables. If the data points tend to form an upward-sloping pattern, it suggests a positive correlation, meaning that as one variable increases, the other also increases. Conversely, a negative correlation is indicated by a downward-sloping pattern, where an increase in one variable corresponds to a decrease in the other. A random scattering of points with no clear pattern suggests there is little to no correlation between the two variables.\nThe density and spread of the points can also give clues about the relationship’s strength. A tight cluster of points along a clear trend line suggests a strong relationship, whereas a more dispersed pattern points to a weaker relationship. Outliers, or points far removed from the general cluster, can signal unusual observations that may need further examination."
  },
  {
    "objectID": "pages/interpreting_outputs/introduction.html",
    "href": "pages/interpreting_outputs/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Analytics for the Australian Grain Industry (AAGI) delivers high-quality analysis, research and development, consultancy, and capacity-building services to the Australian grains research, development and extension (RD&E) sector. Our mission is to help Australian grain growers enhance profitability and competitiveness on the global stage by providing actionable insights from data analysis.\nThis guide is specifically designed to help you interpret the results and outputs provided by AAGI. Understanding the results of statistical analysis is essential for making data-driven decisions, and our goal is to ensure that AAGI analysis outputs are both clear and actionable.\nIf you have any questions or need assistance with interpreting specific aspects of your analysis, the AAGI team is here to support you.\nWhile this guide aims to clarify common outputs, data interpretation is an ongoing process. We are always available to help you fully understand the “what,” “how,” and “why” behind the analysis, ensuring that you can extract the maximum value from your data."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html",
    "title": "Parametric Significance Tests",
    "section": "",
    "text": "Parametric significance tests assess whether model parameters contribute meaningfully to explaining the variability in data. These tests are crucial for determining whether observed differences are statistically valid and help guide decisions on which factors should be included in further analysis.\nThe choice of test depends on the type of data, the research question, the dataset’s structure, and the assumptions underlying the statistical model. These tests are typically used when the data is assumed to follow a certain distribution (e.g., normal distribution) and often involve comparing means or evaluating relationships between variables."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#simpler-methods-for-comparing-group-means",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#simpler-methods-for-comparing-group-means",
    "title": "Parametric Significance Tests",
    "section": "Simpler Methods for Comparing Group Means",
    "text": "Simpler Methods for Comparing Group Means\nt-tests and ANOVA are commonly used in trial data analysis where the predictor variables are categorical, and the response variable is continuous. These tests compare the mean of each group’s response data to identify whether there are significant differences between groups. Both tests operate by comparing observed data with a fitted model and assume that the residuals (the differences between each observed value and the predicted group mean) are approximately normally distributed. This assumption is crucial because the reliability of the tests depends on the normality of the residuals, which is influenced by the underlying distribution of the raw data. If the residuals deviate significantly from normality, the results may be misleading.\nFitting a Linear Model: When fitting a linear model using ANOVA or a t-test, the model generates predicted values for each observation, based on the entire dataset and accounting for the effects of predictor variables (e.g., treatments or environmental conditions). These predicted values represent the expected value for each group, reflecting both overall trends and specific conditions for each group. By incorporating these group-level patterns, the model helps smooth out irregularities in the residuals, making them more likely to follow a normal distribution—even when the raw data itself does not. This smoothing effect helps ensure that the assumption of normality is more likely to hold after fitting the model.\nWhile ANOVA and t-tests can be applied to discrete data such as weed counts, emergence counts, or panicle counts if the data follows a relatively normal distribution, these tests are generally not the best choice for count data, even when the data is normally distributed. Count data often follows non-normal distributions, such as Poisson or negative binomial, which violate the normality assumption required by these tests. In such cases, generalized linear models (GLMs) are a more suitable alternative, as they can model the specific distributional properties of count data (e.g., Poisson or negative binomial), leading to more reliable and accurate results.\nChallenges in Agricultural Research: While t-tests and ANOVA are effective in controlled experimental designs with carefully managed conditions, they are less suitable for on-farm experiments, where environmental factors are often uncontrollable and vary significantly. Agricultural data is typically more complex, involving repeated measurements, hierarchical structures (e.g., fields, plots, and individual plants), and interactions between treatment and environmental variables. These complexities introduce correlations between observations or non-independence within groups, which t-tests and ANOVA are not designed to handle. To account for these factors and ensure valid conclusions, more advanced statistical methods, such as mixed-effects models, are often required.\nt-tests and ANOVA are used in the analysis of trial data whwere the predictor variables are categorical and the response data is contiuous and function by comparing the means of different groups to identify significant differences between them. Both tests operate by comparing the observed data against a fitted model, and they assume that the residuals (the differences between the observed values and the model’s predicted values) are normally distributed. This assumption is important because it underpins the reliability of these tests. If the residuals deviate significantly from normality, the results of these tests may be misleading.\nThe process of fitting a linear model, whether using ANOVA or a t-test, can help mitigate the impact of skewness or extreme values in the raw data. These tests essentially create a model that predicts the group means based on the observed data (is this all the groups together or group by group??), helping to smooth out any irregularities in the data (is this because it’s including the other predictors effects in the predicted value of each group??. In doing so, the residuals — the differences between each observed value and its group’s predicted mean — may become more normally distributed, even when the raw data itself does not follow a normal distribution. This process can help to approximate normality in the residuals, which is why the assumptions of normality are more likely to hold after fitting the model. However, some data such as count data (e.g., weed counts, emergence counts, or panicle counts), which often follows non-normal distributions like the Poisson or negative binomial distributions are sometimes too not normally distributed for anova or t-test to mitigatre against their effect ont eh residuals.\nTo clarify, t-tests and ANOVA work by comparing the means of different groups based on the data at hand. For example, ANOVA compares group means by fitting a linear model to data with categorical predictors (e.g., comparing the means of different treatment groups). This linear model calculates predicted group means, and the residuals reflect the difference between the observed values and these predicted means.\nHowever, there are certain situations where the process may not be enough to ensure that the residuals are normally distributed. When data is highly skewed or contains extreme values (i.e., long tails), or when the sample size within each group is small (typically less than 30, which is often the case in small-plot agricultural trials), assessing the normality of residuals becomes more difficult. Smaller sample sizes mean there’s less data to reliably detect patterns of non-normality, which can compromise the validity of the test results if the residuals are not approximately normally distributed or if normality cannot be clearly assessed.\nFurthermore, while t-tests and ANOVA are effective when used in controlled experimental designs with carefully managed conditions, they are less suitable for on-farm experiments where environmental factors are often uncontrolled. Agricultural data can also be more complex, involving repeated measurements, hierarchical structures (e.g., multiple levels of observations such as fields, plots, and plants), and potential interactions between variables like treatment and environmental effects. These complexities cannot be fully captured by the standard t-test or ANOVA models. As a result, these methods may not adequately address the correlation between observations or the non-independence of data points within plots, which are common in real-world agricultural experiments.\nTests such as t-tests and ANOVA are commonly used to compare the means of different groups. These tests assume that the residuals (the differences between the observed values (raw data) and the model’s fitted values) are normally distributed.\nWhile the process of fitting a linear model (such as ANOVA or a t-test) reduces the impact of skewness or extreme values in the raw data by focusing on group-level patterns and generating predicted means. As a result, the residuals (the differences between the observed values and these predicted means) may become more normally distributed, even when the raw data is not. This smoothing effect helps to approximate normality in the residuals, making the assumptions of normality more likely to hold.\nWhile the process of fitting a linear model with ANOVA or t-test can help reduce the impact of skewness or long tails in the raw data, leading to more normal residuals, the likelihood of non-normal residuals or uncertainty about their distribution increases when the data is highly skewed, has heavy tails, or when the sample size within each group is small (often less than 30, which is common in small-plot trials). Smaller sample sizes make it more difficult to assess the normality of residuals, as there may not be enough data to reliably detect patterns of non-normality. As a result, the validity of test results may be compromised if the residuals are not approximately normally distributed or if the assessment of normality is unclear.\nMoreover, while t-tests and ANOVA are effective in controlled experimental designs with tightly managed conditions, they are less suitable for on-farm experiments where environmental factors often play a significant role and are challenging to control. These tests also do not account for more complex data structures, such as hierarchical data (e.g., multiple levels of observation like fields, plots, and plants) or interactions between variables (e.g., environmental and treatment effects). Furthermore, agricultural data often involves repeated measurements, correlations between observations, or non-independence within plots, which these tests cannot adequately address."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#t-test",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#t-test",
    "title": "Parametric Significance Tests",
    "section": "t-test",
    "text": "t-test\nThe t-test compares the means of two groups to determine if the difference between them is statistically significant.\n\nindependent t-test Compares means of two independent groups.\nPaired t-test Compares two measurements taken from the same subjects (before/after treatment, treatment/control).\n\nOutput & Interpretation:\n\nt-statistic: Measures the difference between group means relative to the variability in the data. Larger values indicate stronger differences between the groupss\nDegrees of freedom (df): Represents the sample size adjusted for the number of estimated parameters. It influences the reliability of the test.\nP-value: A p-value ≤ 0.05 indicates that the means differ significantly, suggesting the observed difference is unlikely due to chance. A p-value &gt; 0.05 suggests the means are not significantly different, and any observed variation is likely due to random fluctuations."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#f-test-anova",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#f-test-anova",
    "title": "Parametric Significance Tests",
    "section": "F-test (ANOVA)",
    "text": "F-test (ANOVA)\nThe F-test is used to compare the means of three or more groups, assessing whether there are statistically significant differences between them. It compares variance between groups to variance within groups to determine if the observed differences are meaningful.\n\nOne-way ANOVA: Tests whether mean differences exist among multiple independent groups based on a single factor.\nTwo-way ANOVA: Evaluates the effects of two factors simultaneously, considering their interactions, to determine how they influence the response variable.\n\nOutputs & Interpretation:\n\nF-statistic: Measures the ratio of variance between groups to variance within groups. A larger F-value suggests stronger evidence that group means are different.\nDegrees of freedom (df): Split into the model (number of groups minus one) and residuals (remaining data points not accounted for by the groups).\nP-value:\n\np-value ≤ 0.05 suggests at least one group differs significantly, indicating that the observed differences are unlikely due to chance.\np-value &gt; 0.05 suggests that the differences across groups may be random and not due to a meaningful effect."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#tests-for-categorical-or-ordinal-data",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#tests-for-categorical-or-ordinal-data",
    "title": "Parametric Significance Tests",
    "section": "Tests for Categorical or Ordinal Data:",
    "text": "Tests for Categorical or Ordinal Data:\nTests for categorical or ordinal data are used to examine relationships between variables where the response is either categorical (nominal) or ordinal.\nCategorical data, also known as nominal data, consists of two or more categories without any intrinsic ordering. For example, binary responses such as disease presence or absence, or a list of two or more weed species or diseases present in each plot. In this case, the categories are simply labels with no meaningful numerical value or ranking attached.\nOrdinal data is similar to categorical data, but the categories have a clear inherent order. However, unlike interval data, the distances between categories are not necessarily both standardised and uniform. For example, disease severity ratings or growth stage scales. A disease severity rating might rely on observational assessments whereby each plot is assigned a number based on the observers subjective assessment of whether the appropriate rating is 1 = no disease, 2 = mild, 3 = severe - in this example different observers may subjectively assign different ratings for the same plots and it is not necessarily expected that the difference between 1 “no disease” and 2”mild” is the same as thd difference betwen 2”mild and 3”severe so the data is neither standardised or uniform. The Zadoks’ Growth Stage Scale, on the other hand follows a consistent framework for describing plant development to help ensure that categories are definied or applied in a standardised way. However, this doesn’t account for growth that sits between specified categories and the intervals between stages are not necessarily uniform; the amount of growth or time between stages can vary depending on factors like plant species, environmental conditions, or specific growing circumstances. For example, the difference between Z1 (main shoot leaf production) and Z2 (tiller production) may not be the same as the difference between Z3 (stem elongation) and Z4 (booting). therefore while Zadok’s is a more standardised measure, and uniformity is improved, the Zadoks scale is considered ordinal in nature. While these stages allow us to rank the categories, calculating an “average” value is not meaningful due to the non-uniformity of the intervals.\nOrdinal data is similar to categorical data, but the categories have a clear, inherent order. However, unlike interval data, the distances between categories are not uniform or standardised. For example, disease severity ratings or growth stage scales, such as the Zadoks Growth Stage Scale, assign numbers to categories (e.g., 1 = no disease, 2 = mild, 3 = severe) or stages of plant growth. Although these stages follow a logical sequence reflecting physical development, the intervals between them are not necessarily equal. For instance, the difference between Z1 (main shoot leaf production) and Z2 (tiller production) may not be the same as the difference between Z3 (stem elongation) and Z4 (booting). As a result, while ordinal scales allow us to rank categories, calculating an “average” value is not meaningful.\nNote: Ordinal data often involves subjective or relative measures, and calculating means can be misleading because the intervals between categories may not be consistent.\nImportance of Proper Analysis For both categorical and ordinal data, comparing group means is inappropriate. For example, computing the average disease severity across plots doesn’t make sense because the levels (e.g., mild vs. severe disease) are not equidistant. Instead, tests for categorical or ordinal data focus on comparing groups or categories using methods that examine the distribution of counts or the association between categories.\nChi-Squared Test The chi-squared test is used to analyse categorical data and determine whether observed frequencies differ significantly from expected values. There are two primary applications:\nGoodness-of-Fit Test: Assesses whether the distribution of a single categorical variable matches an expected distribution. For example, it can evaluate whether seed germination rates follow predicted proportions across different environmental conditions. The expected distribution can be based on previous knowledge, literature, or predefined hypotheses. This doesn’t always assume an even split of categories—it can accommodate expected distributions that reflect different proportions for each category.\nChi-Squared Test for Independence: Tests whether two categorical variables are associated. For example, it can determine if fertilizer type is related to crop growth categories.\nOutput & Interpretation:\nChi-square statistic: Measures the difference between observed and expected frequencies in categorical data. Higher values suggest stronger deviations from expected values.\nDegrees of freedom (df): Calculated based on the number of categories minus constraints, determining the flexibility in comparing distributions.\nP-value:\nA p-value ≤ 0.05 indicates a significant difference or association, meaning the observed data does not match the expected values (Goodness-of-Fit) or that the variables are likely related (Independence Test).\nA p-value &gt; 0.05 implies the observed differences or associations are likely due to random variation, rather than a meaningful effect.\nz-test The z-test is often used to compare proportions in categorical data, especially when sample sizes are large and the population variances are known. It can test whether the observed proportions in different categories significantly differ from expected proportions. The expected proportions are typically based on known population values or theoretical assumptions, such as those derived from previous research or established models.\nFor example, in the context of a two-sample z-test, the expected proportion may come from existing literature, where a specific difference between two groups (e.g., 70% success rate for one treatment and 50% for another) has been well-established.\nOutput & Interpretation:\nz-score: Measures how far a sample proportion deviates from the population proportion in standard deviations.\nP-value:\nA p-value ≤ 0.05 suggests a significant difference in proportions between groups.\nA p-value &gt; 0.05 indicates no meaningful difference.\nTests for categorical or ordinal data are used to examine relationships between variables where the response is either categorical (nominal) or ordinal.\nCategorical data (sometimes called nominal data) consists of two or more categories with no intrinsic ordering. For example, binary responses such as disease presence or absence, or a list or two or more weed species or diseases present in each plot. In these cases, the categories are simply labels with no ranking or meaningful numerical value attached.\nOrdinal data is similar to categorical data, but the categories have a clear order. However, the distances between categories are not uniform or standardised. For instance, disease ratings or growth stage scales might assign numbers to levels (e.g., 1 = no disease, 2 = mild, 3 = severe), but the difference between the categories is not necessarily equal. For example, the difference in disease severity between “mild” (2) and “severe” (3) may not be the same as the difference between “none” (1) and “mild” (2). These scales allow us to rank the categories but do not make it meaningful to calculate the “average” value.\nNote: Ordinal data often involves subjective or relative measures, and calculating means can be misleading because the intervals between categories may not be consistent.\nFor both types of data, comparing group means is not appropriate. For example, it would not make sense to compute the average disease presence across plots because the levels (e.g., mild vs. severe disease) are not equidistant. Tests for categorical or ordinal data, therefore, focus on comparing groups or categories using other methods, such as examining the distribution of counts or the association between different categories."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#chi-squared-test",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#chi-squared-test",
    "title": "Parametric Significance Tests",
    "section": "Chi-Squared Test",
    "text": "Chi-Squared Test\nThe chi-squared test is used to analyse categorical data and determine whether observed frequencies differ significantly from expected values. There are two primary applications:\n\nGoodness-of-Fit Test: Assesses whether the distribution of a single categorical variable matches an expected distribution. For example, it can evaluate whether seed germination rates follow predicted proportions across different environmental conditions.\nChi-Squared Test for Independence: Tests whether two categorical variables are associated. For example, it can determine if fertiliser type is related to crop growth categories.\n\nOutput & Interpretation:\n\nChi-square statistic: Measures the difference between observed and expected frequencies in categorical data. Higher values suggest stronger deviations from expected values.\nDegrees of freedom (df): Calculated based on the number of categories minus constraints, determining the flexibility in comparing distributions.\nP-value:\n\nA p-value ≤ 0.05 indicates a significant difference or association, meaning the observed data does not match the expected values (Goodness-of-Fit) or that the variables are likely related (Independence Test).\nA p-value &gt; 0.05 implies the observed differences or associations are likely due to random variation, rather than a meaningful effect."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#z-test",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#z-test",
    "title": "Parametric Significance Tests",
    "section": "z-test",
    "text": "z-test\nThe z-test is often used to compare proportions in categorical data, especially when sample sizes are large and the population variances are known. It can test whether the observed proportions in different categories significantly differ from expected proportions.\nOutput & Interpretation\n\nz-score: Measures how far a sample proportion deviates from the population proportion in standard deviations.\nP-value:\n\nA p-value ≤ 0.05 suggests a significant difference in proportions between groups.\nA p-value &gt; 0.05 indicates no meaningful difference."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#model-based-methods-for-complex-data",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#model-based-methods-for-complex-data",
    "title": "Parametric Significance Tests",
    "section": "Model-Based Methods for Complex Data:",
    "text": "Model-Based Methods for Complex Data:\nModel-based methods are more advanced tests that require fitting statistical models to the data. These tests are especially useful for analysing complex relationships, including hierarchical structures and interactions. They are also effective when dealing with non-normal data distributions or real-world variability, such as in agricultural trials where environmental influences are at play."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#wald-test",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#wald-test",
    "title": "Parametric Significance Tests",
    "section": "Wald test",
    "text": "Wald test\nThe Wald test assesses individual model parameters to determine whether specific predictors significantly influence the response variable. It is suitable for both binary and continuous response variables.\nOutput & Interpretation:\n\nChi-square statistic: Evaluates whether model coefficients differ significantly from sero. Larger values suggest greater evidence against the null hypothesis.\nDegrees of freedom (df): Represents the number of parameters being tested.\nP-value:\n\nA p-value ≤ 0.05 suggests a predictor significantly influences the response variable.\nA p-value &gt; 0.05 indicates no meaningful impact."
  },
  {
    "objectID": "pages/interpreting_outputs/parametric_significance_tests.html#likelihood-ratio-test",
    "href": "pages/interpreting_outputs/parametric_significance_tests.html#likelihood-ratio-test",
    "title": "Parametric Significance Tests",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe Likelihood Ratio Test compares two nested models to determine whether adding predictors improves the model’s explanatory power. This test is often more reliable than the Wald test when sample sizes are small or when model complexity is high.\nOutput & Interpretation:\n\nLikelihood ratio statistic: Compares how well two models fit the data. Larger values indicate better improvements in model fit with the added predictors.\nDegrees of freedom (df): Reflects the difference in complexity between the two models.\nP-value:\n\np-value ≤ 0.05 suggests adding the predictor significantly improves the model’s performance.\np-value &gt; 0.05 indicates that removing the predictor does not weaken the model, meaning it may not be necessary."
  }
]